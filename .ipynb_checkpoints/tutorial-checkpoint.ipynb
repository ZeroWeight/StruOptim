{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure Optimization Toolbox\n",
    "\n",
    "This toolbox is a network structure optimizer based on CNTK\n",
    "The user is considered to have some basic python skill and know a little about CNTK\n",
    "This tutorial is based on the [MNIST tutorial](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reader\n",
    "\n",
    "We are assuming that the dataset is transferred to the toolbox using the `cntk.io.MinibatchSource` object,\n",
    "which is the default one in the original tutorial. We need to use `sklearn` to split the train dataset into \n",
    "train and the valid one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import sklearn.model_selection\n",
    "import cntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "with open('../MNIST/Train-28x28_cntk_text.txt') as f:\n",
    "    for line in f:\n",
    "        arr.append(line)\n",
    "train,valid = sklearn.model_selection.train_test_split(arr,test_size=1.0/6.0)\n",
    "\n",
    "with open('../MNIST/Train_Split-28x28_cntk_text.txt','w') as f:\n",
    "    for line in train:\n",
    "        f.write(line)\n",
    "with open('../MNIST/Valid_Split-28x28_cntk_text.txt','w') as f:\n",
    "    for line in valid:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reader(path):\n",
    "    label_stream = cntk.io.StreamDef(field='labels', shape=10, is_sparse=False)\n",
    "    feature_stream = cntk.io.StreamDef(field='features', shape=28 * 28, is_sparse=False)\n",
    "    deserializer = cntk.io.CTFDeserializer(path,cntk.io.StreamDefs(\n",
    "        labels=label_stream, features=feature_stream))\n",
    "    return cntk.io.MinibatchSource(deserializer, randomize=True, max_sweeps=cntk.io.INFINITELY_REPEAT)\n",
    "\n",
    "train_reader = create_reader('../MNIST/Train_Split-28x28_cntk_text.txt')\n",
    "valid_reader = create_reader('../MNIST/Valid_Split-28x28_cntk_text.txt')\n",
    "test_reader = create_reader('../MNIST/Test-28x28_cntk_text.txt')\n",
    "\n",
    "network_input = cntk.input_variable((1, 28, 28))\n",
    "network_label = cntk.input_variable(10)\n",
    "\n",
    "def mapping(reader):\n",
    "    return {\n",
    "        network_label : reader.streams.labels,\n",
    "        network_input : reader.streams.features\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition and the Training method\n",
    "\n",
    "Instead of the constant parameters used in the original tutorial, we use an array of paras to define the network and the trainer\n",
    "\n",
    "Here, we define a early-stopping training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(para, verbose = False):\n",
    "    with cntk.layers.default_options(init=cntk.glorot_uniform(), activation=cntk.ops.relu):\n",
    "        # In order to accelerate the debugging step, we choose a simple structure with only 2 parameters\n",
    "        h = cntk.layers.Convolution2D(filter_shape=(5, 5), num_filters=para[0],\n",
    "                                      strides=(1, 1), pad=True, name = 'C1')(network_input/255.0)\n",
    "        h = cntk.layers.layers.MaxPooling(filter_shape=(5, 5), strides=(2, 2),)(h)\n",
    "        \n",
    "        h = cntk.layers.Convolution2D(filter_shape=(5, 5), num_filters=para[1],\n",
    "                                      strides=(1, 1), pad=True, name = 'C2')(h)\n",
    "        h = cntk.layers.layers.MaxPooling(filter_shape=(5, 5), strides=(2, 2))(h)\n",
    "        \n",
    "        z = cntk.layers.Dense(10, activation=None,name = 'R')(h)\n",
    "        loss = cntk.cross_entropy_with_softmax(z, network_label)\n",
    "    label_error = cntk.classification_error(z, network_label)\n",
    "    lr_schedule = cntk.learning_rate_schedule(0.1, cntk.UnitType.minibatch)\n",
    "    learner = cntk.momentum_sgd(z.parameters, lr_schedule,cntk.momentum_schedule(0.9))\n",
    "    trainer = cntk.Trainer(z, (loss, label_error), [learner])\n",
    "    if verbose: log = cntk.logging.ProgressPrinter(100)\n",
    "    for _ in xrange(500):\n",
    "        data = train_reader.next_minibatch(100, input_map=mapping(train_reader))\n",
    "        trainer.train_minibatch(data)\n",
    "        if verbose: log.update_with_trainer(trainer)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[   1- 100]: loss = 1.384290 * 10000;\n",
      " Minibatch[ 101- 200]: loss = 0.254571 * 10000;\n",
      " Minibatch[ 201- 300]: loss = 0.142234 * 10000;\n",
      " Minibatch[ 301- 400]: loss = 0.114502 * 10000;\n",
      " Minibatch[ 401- 500]: loss = 0.109314 * 10000;\n"
     ]
    }
   ],
   "source": [
    "trainer = create_network([6,16],verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Performance Checker\n",
    "\n",
    "Here is a Network Performance Checker to check the time cost and accuracy of the network, and we furthermore\n",
    "implemented a class for saving the Network Performance for later use. \n",
    "\n",
    "#### Constructor arguments\n",
    "\n",
    "- `para`: parameters\n",
    "- `creator`: creator and trainer of the network\n",
    "- `valid`: validation dataset\n",
    "- `mapping`: dict mapping\n",
    "- `valid_batch`: the batch size of the validation set\n",
    "- `valid_iter`: the num of batch in the validation set\n",
    "- `input_key`: key to the dict of the inputs\n",
    "\n",
    "#### Members\n",
    "- `para`: parameters\n",
    "- `accuracy`: validation set accuracy\n",
    "- `time`: cpu computing time, using time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,16      \t7.14e+01ms\t96.24%\n"
     ]
    }
   ],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,para,creator,valid,mapping,valid_batch,valid_iter,input_key):\n",
    "        self.para = para\n",
    "        \n",
    "        network = creator(para)\n",
    "        temp_err = 0\n",
    "        for i in range(valid_iter):\n",
    "            data = valid.next_minibatch(valid_batch, input_map = mapping(valid))\n",
    "            temp_err += network.test_minibatch(data)\n",
    "        self.accuracy = 1 - temp_err/ valid_iter\n",
    "        \n",
    "        model_name = os.path.join('module','_'.join(map(str,para)))\n",
    "        network.model.save(model_name)\n",
    "        cpu_timer = cntk.load_model(model_name,device = cntk.cpu())\n",
    "        \n",
    "        time_cost = []\n",
    "        for i in range(valid_iter):\n",
    "            data = valid.next_minibatch(valid_batch,input_map = mapping(valid))\n",
    "            arr = numpy.array(data[input_key].as_sequences())\n",
    "            arr = numpy.reshape(arr,(-1,) + input_key.shape)\n",
    "            #print arr.shape\n",
    "            current_time = time.clock()\n",
    "            cpu_timer.eval(arr,device = cntk.cpu())\n",
    "            current_time = time.clock() - current_time\n",
    "            time_cost.append(current_time)\n",
    "        self.time =  numpy.min(time_cost)\n",
    "    def __str__(self):\n",
    "        return \"{0:10s}\\t{1:.2e}ms\\t{2:.2f}%\".format(','.join(map(str,self.para)),self.time*1000,self.accuracy*100)\n",
    "\n",
    "print Node([6,16],create_network,valid_reader,mapping,100,100,network_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Optimization Function\n",
    "\n",
    "User should pass the self-defined network parameters into the function\n",
    "\n",
    "- `start_point`: the smallest network structure\n",
    "- `end_point`: the biggest network structure\n",
    "- `time_limit`: the time limit of the network structure\n",
    "- `creator`: network creation and training method\n",
    "- `train`: train reader\n",
    "- `valid`: valid reader\n",
    "- `mapping`: reader mapping\n",
    "- `valid_batch`: the batch size of the validation set\n",
    "- `valid_iter`: the num of batch in the validation set\n",
    "- `input_key`: key to the dict of the inputs\n",
    "- `encoder`: by default is the bi_encoder, can be the self defined one\n",
    "- `decoder`: by default is the bi_decoder, can be the self defined one\n",
    "- `device`: device you want to used to train the model, default is GPU #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10759999999999958, 0.029198999999998421]\n"
     ]
    }
   ],
   "source": [
    "start_point = [1,1]\n",
    "end_point = [128,128]\n",
    "def _node(para_list):\n",
    "    n =  Node(para_list,create_network,valid_reader,mapping,100,100,network_input)\n",
    "    return [n.accuracy,n.time]\n",
    "print _node([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9764, 0.49342799999999443], [0.9777, 0.11970600000000786], [0.9791, 0.94499199999998496], [0.9679, 0.8312090000000012], [0.9734, 0.61528799999996409], [0.973, 1.0257190000000378], [0.9732000000000001, 0.28661299999998846], [0.9757, 0.89857799999992949], [0.9738, 0.85432800000000952], [0.9785, 0.54590199999995548], [0.9751, 0.80834300000003623], [0.9779, 0.94682299999999486], [0.9753000000000001, 0.57694500000002336], [0.10759999999999992, 0.051626999999825784], [0.9724, 0.067759000000023661], [0.9783999999999999, 0.81671200000005229], [0.9645, 0.76751800000010917], [0.9777, 0.84131300000012743], [0.981, 0.70391999999992549], [0.9726, 0.46140100000002349]]\n"
     ]
    }
   ],
   "source": [
    "hist = []#para\n",
    "hist_perform = []\n",
    "sample = 20\n",
    "sample = min(sample,numpy.min(numpy.array(end_point) - numpy.array(start_point)))\n",
    "sample_hist = []\n",
    "for i in range(len(start_point)):\n",
    "    _range = range(start_point[i],end_point[i])\n",
    "    inc = random.sample(_range,sample)\n",
    "    sample_hist.append(inc)\n",
    "for i in range(sample):\n",
    "    para = []\n",
    "    for j in range(len(start_point)):\n",
    "        para.append(sample_hist[j][i])\n",
    "    hist.append(para)\n",
    "for para in hist:\n",
    "    hist_perform.append(_node(para))\n",
    "print hist_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120, 26]]\n"
     ]
    }
   ],
   "source": [
    "def next_gen_linear(para_list,end_point,step = 16):\n",
    "    group = []\n",
    "    for i in xrange(len(para_list)):\n",
    "        new_list = list(para_list)\n",
    "        new_list[i] += step\n",
    "        if new_list  <= end_point:\n",
    "            group.append(new_list)\n",
    "    return group\n",
    "            \n",
    "print  next_gen_linear([120,10],end_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 128], [2, 128], [4, 128]]\n"
     ]
    }
   ],
   "source": [
    "def gen_cut(des,ori,step = 3):\n",
    "    group = []\n",
    "    for i in xrange(len(des)):\n",
    "        if des[i] != ori[i]:\n",
    "            if des[i] - ori[i] == 1:\n",
    "                break\n",
    "            _step = min(step,des[i] - ori[i] - 1)\n",
    "            _range = range(des[i] - ori[i] - 1)\n",
    "            inc = random.sample(_range,_step)\n",
    "            for s in inc:\n",
    "                new = list(ori)\n",
    "                new[i] += s + 1\n",
    "                group.append(new)\n",
    "            break\n",
    "                \n",
    "    return group\n",
    "\n",
    "print gen_cut([17,128],[1,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39.306958342333225, 12.955227072519499)\n"
     ]
    }
   ],
   "source": [
    "def get_mu(para):\n",
    "    group_arr = numpy.array(hist)\n",
    "    Y = numpy.array(hist_perform)\n",
    "    T = Y[:,1]\n",
    "    Y = Y[:,0]\n",
    "    \n",
    "    \n",
    "    features = group_arr.shape[1]\n",
    "    samples = group_arr.shape[0]\n",
    "\n",
    "    data  = numpy.zeros((group_arr.shape[0],0))\n",
    "    for i in range(features):\n",
    "        for j in range(i,features):\n",
    "            item =  group_arr[:,j] * group_arr[:,i]\n",
    "            item = numpy.reshape(item,(group_arr.shape[0],1))\n",
    "            data = numpy.concatenate((data,item),axis = 1)\n",
    "    data = numpy.concatenate((data,numpy.ones((group_arr.shape[0],1)),group_arr),axis = 1)\n",
    "\n",
    "    w_para = []\n",
    "    for i in range(features):\n",
    "        for j in range(i,features):\n",
    "            w_para.append(para[i] * para[j])\n",
    "    w_para.append(1)\n",
    "    w_para = w_para + para\n",
    "    w_para = numpy.array(w_para)\n",
    "    w_features = data.shape[0]\n",
    "\n",
    "    tao = 1e1\n",
    "    cost = 0\n",
    "    delta = numpy.zeros(w_features)\n",
    "\n",
    "    for i in range(w_features):\n",
    "        delta[i] = numpy.exp(-0.5 * numpy.linalg.norm(group_arr[i,:] - numpy.array(para),2) / tao / tao)\n",
    "\n",
    "    W = numpy.diag(delta)\n",
    "    W = W / numpy.sum(W)\n",
    "    U = numpy.mat(data)\n",
    "    Y = numpy.mat(Y).transpose()\n",
    "    T = numpy.mat(T).transpose()\n",
    "    theta_P = numpy.linalg.pinv(U.transpose() * W * U) * U.transpose() * W * Y\n",
    "    theta_T = numpy.linalg.pinv(U.transpose() * W * U) * U.transpose() * W * T\n",
    "    alpha_P = numpy.zeros((features,features))\n",
    "    alpha_T = numpy.zeros((features,features))\n",
    "    idx = 0\n",
    "    for i in range(features):\n",
    "        for j in range(i,features):\n",
    "            alpha_P[j,i] = theta_P[idx]\n",
    "            alpha_T[j,i] = theta_T[idx]\n",
    "            idx = idx + 1\n",
    "    idx = idx + 1\n",
    "    beta_P = numpy.zeros((features,1))\n",
    "    beta_T = numpy.zeros((features,1))\n",
    "    for i in range(features):\n",
    "        beta_P[i] = theta_P[idx]\n",
    "        beta_T[i] = theta_T[idx]\n",
    "        \n",
    "    alpha_P = alpha_P + alpha_P.transpose()\n",
    "    alpha_T = alpha_T + alpha_T.transpose()\n",
    "    PP =  numpy.array(numpy.mat(alpha_P) * numpy.mat(para).transpose() + beta_P)\n",
    "    PT =  numpy.array(numpy.mat(alpha_T) * numpy.mat(para).transpose() + beta_T)\n",
    "    mu_P = []\n",
    "    mu_T = []\n",
    "    for c in  itertools.combinations(range(features), 2):\n",
    "        mu_P.append(alpha_P[c[0],c[0]] / PP[c[0]] / PP[c[0]])\n",
    "        mu_P.append(alpha_P[c[0],c[1]] / PP[c[0]] / PP[c[1]])\n",
    "        mu_P.append(alpha_P[c[1],c[1]] / PP[c[1]] / PP[c[1]])\n",
    "        mu_T.append(alpha_T[c[0],c[0]] / PT[c[0]] / PP[c[0]])\n",
    "        mu_T.append(alpha_T[c[0],c[1]] / PT[c[0]] / PP[c[1]])\n",
    "        mu_T.append(alpha_T[c[1],c[1]] / PT[c[1]] / PP[c[1]])\n",
    "    muP = max(*mu_P)[0]\n",
    "    muT = min(*mu_T)[0]\n",
    "    return muP,muT\n",
    "    \n",
    "print (get_mu([20,20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272207.0240193699, 229821.46883368283)\n"
     ]
    }
   ],
   "source": [
    "def gird_mu(count = 10000):\n",
    "    a = []\n",
    "    for i in range(len(para)):\n",
    "        a.append(random.sample(range(start_point[i],end_point[i]+1),int(numpy.power(count,1.0/len(para))) ))\n",
    "\n",
    "    mu = []\n",
    "    for point in itertools.product(*a):\n",
    "        mu.append(get_mu(list(point)))\n",
    "    mu = zip(*mu)\n",
    "    return max(*mu[0]),-min(*mu[1])\n",
    "print gird_mu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One iteration checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_eps = 1e-6\n",
    "current = [6,16]\n",
    "ori = _node(start_point)\n",
    "group = next_gen_linear(current,end_point)\n",
    "performance = []# acc,time\n",
    "for new_para in group:\n",
    "    vec = _node(new_para)\n",
    "    if vec[0] < ori[0]: vec[0] = ori[0]\n",
    "    if vec[1] < ori[1]: vec[1] = ori[1] + t_eps\n",
    "    performance.append(_node(new_para))\n",
    "arr_performance = numpy.array(performance)\n",
    "p_idx = numpy.argmax(arr_performance[:,0])\n",
    "ratio_score = (arr_performance[:,0] - ori[0]) / (arr_performance[:,1] - ori[1])\n",
    "t_idx = numpy.argmax(ratio_score)\n",
    "new_group = gen_cut(group[p_idx],current)\n",
    "if p_idx != t_idx: new_group += gen_cut(group[t_idx],current)\n",
    "for new_para in new_group:\n",
    "    group.append(new_para)\n",
    "    vec = _node(new_para)\n",
    "    if vec[0] < ori[0]: vec[0] = ori[0]\n",
    "    if vec[1] < ori[1]: vec[1] = ori[1] + t_eps\n",
    "    performance.append(_node(new_para))\n",
    "arr_performance = numpy.array(performance)\n",
    "ratio_score = (arr_performance[:,0] - ori[0]) / (arr_performance[:,1] - ori[1])\n",
    "idx = numpy.argmax(ratio_score)\n",
    "output_para = group[idx]\n",
    "hist = hist + group\n",
    "hist_perform = hist_perform + performance\n",
    "print output_para"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
